{"meta":{"title":"Hexo","subtitle":"","description":"","author":"John Doe","url":"http://example.com","root":"/"},"pages":[{"title":"404","date":"2022-05-17T01:05:30.000Z","updated":"2022-05-17T01:07:08.856Z","comments":true,"path":"404.html","permalink":"http://example.com/404.html","excerpt":"","text":""},{"title":"About","date":"2022-05-17T01:04:07.000Z","updated":"2022-05-17T01:05:07.824Z","comments":true,"path":"about/index.html","permalink":"http://example.com/about/index.html","excerpt":"","text":""},{"title":"Categories","date":"2022-05-17T01:00:30.000Z","updated":"2022-05-17T01:03:56.938Z","comments":true,"path":"categories/index.html","permalink":"http://example.com/categories/index.html","excerpt":"","text":""},{"title":"Tags","date":"2022-05-17T01:04:18.898Z","updated":"2022-05-17T01:04:18.898Z","comments":true,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Spark基础环境配置","slug":"Spark基础环境配置","date":"2022-05-17T03:05:00.051Z","updated":"2022-05-17T03:09:36.368Z","comments":true,"path":"2022/05/17/Spark基础环境配置/","link":"","permalink":"http://example.com/2022/05/17/Spark%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/","excerpt":"","text":"本文主 本文主要使用Hexo与Github进行个人blog的搭建Hexo官网：HexoGithub官网：Github 环境介绍本地环境为: Window10系统、Linux虚拟机 开始搭建1.基础环境在开始配置前，需要检查虚拟机主机名、hosts映射、关闭防火墙、免密登录、同步时间等操作 （1）编辑主机名（三台机器）#查看系统主机名(三台主机) $ cat /etc/hostname #在三台主机上更改主机名 #在 master 主节点 $ echo \"master\" >/etc/hostname #在 slave1 节点 $ echo \"slave1\" >/etc/hostname #在 slave2 节点 $ echo \"slave2\" >/etc/hostname （2）hosts映射#查看系统映射 $ cat /etc/hosts #编辑 /etc/hosts 文件 $ vim /etc/hosts #内容修改为 （三台主机内容一致） 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 192.168.88.135 master 192.168.88.136 slave1 192.168.88.137 slave2 （3）关闭防火墙#关闭防火墙 $ systemctl stop firewalld.service #禁止防火墙开启自启 $ systemctl disable firewalld.service （4）免密登录#master 生成公钥私钥，四个回车即可 $ ssh-keygen #master 配置免密登录到master slave1 slave2三台主机 $ ssh-copy-id master $ ssh-copy-id slave1 $ ssh-copy-id slave2 （5）时间同步#安装 ntp $ yum install ntp -y #设置 ntp 开机自启动 $ systemctl enable ntpd &amp;&amp; systemctl start ntpd #三台主机分别运行以下命令 $ ntpdate ntp4.aliyun.com 2.JDK安装（1）下载安装包本文使用的 JDK 是1.8版本jdk1.8安装包下载注意：下载的是后缀为 .tar.gz 的包 （2）在主机 master 上安装 JDK#编译环境软件安装目录 $ mkdir -p /export/server #上传本地下载好的jdk-8u241-linux-x64.tar.gz到/export/server/目录下 并解压文件 $ tar -zxvf jdk-8u241-linux-x64.tar.gz #配置环境变量 $ vim /etc/profile #在文件内添加如下内容 # jdk 环境变量 export JAVA_HOME=/export/server/jdk1.8.0_241 export PATH=$PATH:$JAVA_HOME/bin export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.ja #重新加载环境变量文件 $ source /etc/profile #查看 java 版本号 $ java -version #出现 java version \"1.8.0_241\" 表示安装成功 （3）分发#master 节点将 java 传输到 slave1 和 slave2 $ cd /export/server $ scp -r /export/server/jdk1.8.0_241/ root@slave1:/export/server/ $ scp -r /export/server/jdk1.8.0_241/ root@slave2:/export/server/ #配置 slave1 和 slave2 的 jdk 环境变量（注：和上方 master 的配置方法一样） #配置完成后，在 master slave1 和slave2 三台主机创建软连接 $ cd /export/server $ ln -s jdk1.8.0_241/ jdk #重新加载环境变量文件 $ source /etc/profile 3.Hadoop安装（1）下载安装包本文使用的 hadoop 是3.3.0版本hadoop3.3.0安装包下载注意：下载的是后缀为 .tar.gz的包 （2）在主机 master 上安装 hadoop#上传本地下载好的 hadoop-3.3.0-Centos7-64-with-snappy.tar.gz 到 /export/server 并解压文件 $ tar -zxvf hadoop-3.3.0-Centos7-64-with-snappy.tar.gz #修改配置文件,进入到 hadoop 目录下 $ cd /export/server/hadoop-3.3.0/etc/hadoop #编辑 hadoop-env.sh 文件 $ vim hadoop-env.sh #文件最后添加 export JAVA_HOME=/export/server/jdk1.8.0_241 export HDFS_NAMENODE_USER=root export HDFS_DATANODE_USER=root export HDFS_SECONDARYNAMENODE_USER=root export YARN_RESOURCEMANAGER_USER=root export YARN_NODEMANAGER_USER=root #修改 core-site.xml 文件 $ vim core-site.xml #添加如下内容 &lt;!-- 设置默认使用的文件系统 Hadoop支持file、HDFS、GFS、ali|Amazon云等文件系统 - -> &lt;property> &lt;name>fs.defaultFS&lt;/name> &lt;value>hdfs://node1:8020&lt;/value> &lt;/property> &lt;!-- 设置Hadoop本地保存数据路径 --> &lt;property> &lt;name>hadoop.tmp.dir&lt;/name> &lt;value>/export/data/hadoop-3.3.0&lt;/value> &lt;/property> &lt;!-- 设置HDFS web UI用户身份 --> &lt;property> &lt;name>hadoop.http.staticuser.user&lt;/name> &lt;value>root&lt;/value> &lt;/property> &lt;!-- 整合hive 用户代理设置 --> hdfs-site.xml mapred-site.xml &lt;property> &lt;name>hadoop.proxyuser.root.hosts&lt;/name> &lt;value>*&lt;/value> &lt;/property> &lt;property> &lt;name>hadoop.proxyuser.root.groups&lt;/name> &lt;value>*&lt;/value> &lt;/property> &lt;!-- 文件系统垃圾桶保存时间 --> &lt;property> &lt;name>fs.trash.interval&lt;/name> &lt;value>1440&lt;/value> &lt;/property> #修改 mapred-site.xml 文件 $ vim mapred-site.xml #添加如下内容 &lt;!-- 设置MR程序默认运行模式： yarn 集群模式 local本地模式 --> &lt;property> &lt;name>mapreduce.framework.name&lt;/name> &lt;value>yarn&lt;/value> &lt;/property> &lt;!-- MR程序历史服务地址 --> &lt;property> &lt;name>mapreduce.jobhistory.address&lt;/name> &lt;value>node1:10020&lt;/value> &lt;/property> &lt;!-- MR程序历史服务器web端地址 --> &lt;property> &lt;name>mapreduce.jobhistory.webapp.address&lt;/name> &lt;value>node1:19888&lt;/value> &lt;/property> &lt;property> &lt;name>yarn.app.mapreduce.am.env&lt;/name> &lt;value>HADOOP_MAPRED_HOME=$&amp;#123;HADOOP_HOME&amp;#125;&lt;/value> &lt;/property> &lt;property> &lt;name>mapreduce.map.env&lt;/name> &lt;value>HADOOP_MAPRED_HOME=$&amp;#123;HADOOP_HOME&amp;#125;&lt;/value> &lt;/property> &lt;property> &lt;name>mapreduce.reduce.env&lt;/name> yarn-site.xml workers #修改 hdfs-site.xml 文件 $ vim hdfs-site.xml #添加如下内容 &lt;!-- 设置SNN进程运行机器位置信息 --> &lt;property> &lt;name>dfs.namenode.secondary.http-address&lt;/name> &lt;value>slave1:9868&lt;/value> &lt;/property> #修改 yarn-site.xml 文件 $ vim yarn-site.xml #添加如下内容 &lt;!-- 设置YARN集群主角色运行机器位置 --> &lt;property> &lt;name>yarn.resourcemanager.hostname&lt;/name> &lt;value>node1&lt;/value> &lt;/property> &lt;property> &lt;name>yarn.nodemanager.aux-services&lt;/name> &lt;value>mapreduce_shuffle&lt;/value> &lt;/property> &lt;!-- 是否将对容器实施物理内存限制 --> &lt;property> &lt;name>yarn.nodemanager.pmem-check-enabled&lt;/name> &lt;value>false&lt;/value> &lt;/property> &lt;!-- 是否将对容器实施虚拟内存限制。 --> &lt;property> &lt;name>yarn.nodemanager.vmem-check-enabled&lt;/name> &lt;value>false&lt;/value> &lt;/property> &lt;!-- 开启日志聚集 --> &lt;property> &lt;name>yarn.log-aggregation-enable&lt;/name> &lt;value>true&lt;/value> &lt;/property> &lt;!-- 设置yarn历史服务器地址 --> &lt;property> &lt;name>yarn.log.server.url&lt;/name> &lt;value>http://node1:19888/jobhistory/logs&lt;/value> &lt;/property> &lt;!-- 历史日志保存的时间 7天 --> &lt;property> &lt;name>yarn.log-aggregation.retain-seconds&lt;/name> &lt;value>604800&lt;/value> &lt;/property> #修改 workers 文件 $ vim workers #将 workers 里的 localhost 删除，添加如下内容 master slave1 slave2 （3）分发#master 节点将 hadoop 传输到 slave1 和 slave2 $ cd /export/server $ scp -r hadoop-3.3.0 root@node2:$PWD $ scp -r hadoop-3.3.0 root@node3:$PWD #将 hadoop 添加到环境变量 vim /etc/profile #在文件内添加如下内容 # hadoop 环境变量 export HADOOP_HOME=/export/server/hadoop-3.3.0 export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin #配置 slave1 和 slave2 的 hadoop 环境变量（注：和上方 master 的配置方法一样） #配置完成后，在 master slave1 和slave2 三台主机创建软连接 $ cd /export/server $ ln -s hadoop-3.3.0/ hadoop #重新加载环境变量文件 $ source /etc/profile #在 master 主节点进行 Hadoop 集群启动 格式化 namenode（只有首次启动需要格式化） $ hdfs namenode -format #等待初始化完成后，使用脚本一键起动 $ start-all.sh #起动后，输入jps查看进程号 $ jps #进程查看完毕后可进入到 WEB 界面 #HDFS集群的界面网站是:http://master:9870/ #YARN集群的界面网站是:http://master:9870/ 4.安装zookeeper（1）下载安装包本文使用的 zookeeper 是3.7.0版本zookeeper3.7.0安装包下载注意：下载的是后缀为 .tar.gz 的包,安装包需要3.7版本网上，否者后续spark配置会出现问题 （2）在主机 master 上安装 zookeeper#上传本地下载好的 apache-zookeeper-3.7.0-bin.tar.gz 到 /export/server 并解压文件 $ tar -zxvf apache-zookeeper-3.7.0-bin.tar.gz #修改配置文件,进入到 /export/server 目录下 $ cd /export/server/ #在 /export/server 目录下创建 zookeeper 软连接 $ ln -s apache-zookeeper-3.7.0-bin/ zookeeper #进入到 zookeeper 目录下 $ cd zookeeper #进入到 zookeeper 下的 conf 文件内 $ cd /export/server/zookeeper/conf/ #将 zoo_sample.cfg 文件复制为新文件 zoo.cfg $ cp zoo_sample.cfg zoo.cfg #在 zoo.cfg 文件内添加如下内容 #Zookeeper的数据存放目录 dataDir=/export/server/zookeeper/zkdatas # 保留多少个快照 autopurge.snapRetainCount=3 # 日志多少小时清理一次 autopurge.purgeInterval=1 # 集群中服务器地址 server.1=master:2888:3888 server.2=slave1:2888:3888 server.3=slave2:2888:3888 #进入 /export/server/zookeeper/zkdatas 目录在此目录下创建 myid 文件,将 1 写入进去 $ cd /export/server/zookeeper/zkdata $ mkdir myid $ echo '1' > myid #查看是否成功写入 $ vim myid #出现数字1即为成功 （3）分发#master 节点将 zookeeper 传输到 slave1 和 slave2 $ cd /export/server $ scp -r /export/server/zookeeper/ slave1:$PWD $ scp -r /export/server/zookeeper/ slave2:$PWD #推送完成后将 slave1 和 slave2 的 /export/server/zookeeper/zkdatas/ 文件夹下的 myid中的内容分别改为 2 和 3 #在 slave1 节点上 $ cd /export/server/zookeeper/zkdatas/ $ echo '2' > myid #查看是否成功写入 $ vim myid #出现数字2即为成功 #在 slave2 节点上 $ cd /export/server/zookeeper/zkdatas/ $ echo '3' > myid #查看是否成功写入 $ vim myid #出现数字3即为成功 #将 zookeeper 添加到环境变量 vim /etc/profile #在文件内添加如下内容 # zookeeper 环境变量 export ZOOKEEPER_HOME=/export/server/zookeeper export PATH=$PATH:$ZOOKEEPER_HOME/bin #配置 slave1 和 slave2 的 hadoop 环境变量（注：和上方 master 的配置方法一样） #重新加载环境变量文件 $ source /etc/profile #三台机器分别进入 /export/server/zookeeper/bin 目录下启动 zkServer.sh 脚本 $ cd /export/server/zookeeper/bin $ zkServer.sh start #查看 zookeeper 的状态 $ zkServer.sh status #也可以通过jps查看zookeeper的进程 $ jps 以上,就是Spark基础环境的配置,接下来会带来 Spark local&amp; stand-alone配置","categories":[{"name":"工具","slug":"工具","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[]},{"title":"Spark local & stand-alone配置-中文文档","slug":"Spark local& stand-alone配置","date":"2022-05-17T03:04:54.670Z","updated":"2022-05-17T03:07:33.281Z","comments":true,"path":"2022/05/17/Spark local& stand-alone配置/","link":"","permalink":"http://example.com/2022/05/17/Spark%20local&%20stand-alone%E9%85%8D%E7%BD%AE/","excerpt":"","text":"Spark提供多种运行模式，包括Spark local 模式(单机)、Spark alone 模式(集群)、 hadoop YARN 模式(集群)和 Kubernetes 模式(容器集群) Spark local 模式:以一个独立的进程,通过其内部的多个线程来模拟整个Spark运行时环境来开发和测试 Spark alone 模式:各个角色以独立进程的形式存在,并组成Spark集群环境,运行在linux系统之上 hadoop YARN 模式:Spark中的各个角色运行在YARN的容器内部,并组成Spark集群环境,运行在yarn容器内 Spark中的各个角色运行在Kubernetes的容器内部,并组成Spark集群环境 本文着重描述 Spark local 模式和 Spark alone 模式 Spark local模式Spark local 模式是以一个独立的进程，通过其内部的多个线程来模拟整个Spark运行时的环境Spark由四类角色组成整个Spark的运行环境: Master角色，管理整个集群的资源 Worker角色，管理单个服务器的资源 Driver角色，管理单个Spark任务在运行的时候的工作 Executor角色，单个任务运行的时候的工作者 安装注意：以下操作需要完成 Spark 基础环境配置。具体配置移步到Spark基础环境配置","categories":[{"name":"工具","slug":"工具","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[]}],"categories":[{"name":"工具","slug":"工具","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[]}